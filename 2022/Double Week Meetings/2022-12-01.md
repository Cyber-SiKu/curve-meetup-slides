# Curve 双周会 2022-12-01

## 时间

2022/12/01 19:00-19:30

## 加入会议

```
会议主题：curve双周会
会议时间：2022/11/03 19:00-19:30 (GMT+08:00) 中国标准时间 - 北京
重复周期：2022/10/20-2023/07/27 19:00-19:30, 每两周 (周四)

点击链接入会，或添加至会议列表：
https://meeting.tencent.com/dm/DckjtgWnOM1o

#腾讯会议：493-3764-3146
```

## 会议内容

### 开源版本

**Curve文件存储**

1. 当前稳定版本是 [v2.3.0-rc4](https://github.com/opencurve/curve/tree/v2.3.0-rc4)

- 已经上到生产环境，支撑ES和AI的冷数据迁移业务

- 这两个业务如何使用Curve文件存储以及在使用过程中发现的问题，后续会有相应的公众号文章说明。简要说明：

    - ES对接过程中发现一个功能bug, 在 ```write file -> read parent dir  -> write file -> read parent dir``` 这种写流程下，最后一次操作获取到的```file``` 长度不对，fix [pr2112](https://github.com/opencurve/curve/pull/2112)

    - AI冷数据迁移到CurveFS中发现性能问题，数据迁移的带宽上不去，这跟客户端上传S3的带宽有关，[release2.4](https://github.com/opencurve/curve/tree/release2.4)将aws的库更新到了新版本，使用异步接口，性能有大幅度提升, 当前版本可以通过[加大线程数量](https://github.com/opencurve/curve/blob/5401ebd04bfd8045fbc588a352e715e71a5f7cc4/curvefs/conf/client.conf#L157)提升带宽。

- 感兴趣的小伙伴可以尝试一下这个版本

2. 当前最新的release分支是 [release2.4](https://github.com/opencurve/curve/tree/release2.4)

- 不稳定分支，正在稳定性、功能测试中，[测试方案及进度](https://github.com/opencurve/curve/blob/master/docs/cn/test/release2.4-test.pdf)

- 整体测试项

- [测试问题及进展](https://github.com/opencurve/curve/milestone/8)

3. Doing

- 文件存储新增共享读缓存，加速AI训练场景中训练中间数据的存储和多端共享。对于后端为S3的部署方式，如果需要实现多个client共享，目前数据只有上传到S3，才能对所有client可见，但S3的性能又不尽人意。当前的解决方案是，使用memcache（不仅是全内存，还支持SSD）作为共享的读缓存。

- 文件存储CurveBS后端删除方案。这个功能搁置了比较久，最近开始方案调研了，github上的讨论区：[[CurveFS] Support CurveBS as backend](https://github.com/opencurve/curve/discussions/2117)


**Curve块存储**

1. 社区用户体验不到Curve块存储的标称性能，针对这一问题

- 近期在准备Curve块存储的性能测试指南，包括几个部分：

    - 块存储在 ```3台机器*2盘/台``` 和 ```6台机器*20盘/台``` 两种规模下的参考性能，及该性能和磁盘性能总和的关系

    - 块存储性能相关Metric说明

    - 块存储其他Metric说明

- 性能检测工具（TODO）

2. 社区用户提到的Curve块存储的小IO写放大问题

- Curve块存储的IO放大是影响小IO性能的重要原因。```wal = (header/几十字节 + data/4k对齐)``` ，```wal``` 的写模式为O_DIRECT，对于4k数据这里有 **1倍** 的放大，再加上数据apply，和本地文件系统的元数据更新，整体的写放大在 **3倍** 以上

- 目前 POC 了一个解决方法，```header``` 和 ```data``` 数据分开存放。```data（本来就是4k对齐的）``` 还是以O_DIRECT的方式，```header``` 写 PageCache， 依赖本地文件系统刷盘或者本文件写完刷盘。这种方式可以提升 30% 左右的性能。但是有一个风险，```header``` 数据写缓存，如果三副本同时掉电，数据丢失风险很大。在生产中使用需要进行评估

- 后续还会有更多的优化，可以在社区各种渠道交流

3. 社区用户提到的大IO性能问题

- 大IO在条带化场景下，还是没有特别高的性能优势。这一点正在做全链路的IO性能分析，目前还没有阶段性的进展

4. [RDMA](https://github.com/wu-hanqing/curve/tree/ucx_1.2) 和 [SPDK](https://github.com/opencurve/curve/pull/1991) 的支持

- 正在做 RDMA 和 SPDK 的整合

- 难点在于如何让 ```从网络接受到数据``` 到 ```数据落盘``` 都可以bypass kernel。理想的情况是，有一个用户空间的buffer，RDMA 直接写进来，然后这里的数据直接下到磁盘上。感兴趣的同学环境在社区讨论。

**Curve on K8s**

- 支持 Curve 在 k8s 上的部署， github 的讨论区: [[Curve] Support Operator](https://github.com/opencurve/curve/discussions/2118)

    - k8s 上的部署可以分为客户端和服务单，客户端通常的做法是支持 CSI,这一点 Curve 块和文件都已经支持了

    - 当前要解决的问题是支持服务端在 k8s 上的部署，由于 Curve 集群的拓扑最初设计没有面向 k8s, 所以当前做适配有一些问题要解决。比如 k8s 中，ip/port做了虚拟化，但 Curve 中 ip/port 存储节点（chunkserver/metaserver）的标识，要求不同进程的 ip/port 不相同。另外，braft 的副本配置也是通过 ip/port来标识的。

    - 后续的问题和开发进度都会在Discussion中更新，欢迎感兴趣的小伙伴参与

### 开源活动

**开发者活动**

开发者活动的pr提交的截止日期是1203，有参与活动的小伙伴们注意一下~

开发者活动当前进展：[[Developer Activities]: Call For Participation！](https://github.com/opencurve/curve/issues/2017)

### 答疑
1. tgt的iscsi支持

有计划支持，但优先级不高，目前还没有放在版本中。欢迎社区的小伙伴一起

2. 我也提个小意见：好多issue提的挺早的，或者有些是用户提的问题，但是好几个月一直没关闭或者还没assign 某人跟踪.

关于issue的管理：现在有值周机制:https://github.com/opencurve/community/blob/master/affair/oncall.md
遗留的issue会统一进行处理